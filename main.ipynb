{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Coding Group Project - 2024/25 \n",
    "\n",
    "Michele Turco, Mattia Cervelli, Lorenzo Laterza "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Import Libraries and the Dataset\n",
    "\n",
    "Setup your python environment and download the credit-score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df = pd.read_csv('train_biased.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Understanding the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1) General overview of the dataset\n",
    "\n",
    "Using the function `.head()`, we can extract from the dataset the first x rows (x=5 by default, but we can change this number). They are not useful for analytic purposes, but still enable us to visualize the dataset to get a general overview of the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of the DataFrame:\n",
      "       ID Customer_ID     Month           Name    City         Street    Age  \\\n",
      "0     NaN   CUS_0xd40       NaN  Aaron Maashoh  Lonton  Oxford Street    NaN   \n",
      "1  0x1603   CUS_0xd40  February  Aaron Maashoh  Lonton  Oxford Street   23.0   \n",
      "2  0x1604   CUS_0xd40       NaN            NaN  Lonton  Oxford Street -500.0   \n",
      "3  0x1605   CUS_0xd40     April  Aaron Maashoh  Lonton  Oxford Street   23.0   \n",
      "4     NaN   CUS_0xd40       May  Aaron Maashoh  Lonton  Oxford Street   23.0   \n",
      "\n",
      "           SSN Occupation Annual_Income  ...  Num_of_Delayed_Payment  \\\n",
      "0  821-00-0265    Manager      19114.12  ...                       7   \n",
      "1  821-00-0265    Manager      19114.12  ...                     NaN   \n",
      "2  821-00-0265    Manager      19114.12  ...                       7   \n",
      "3  821-00-0265    Manager      19114.12  ...                       4   \n",
      "4  821-00-0265    Manager      19114.12  ...                     NaN   \n",
      "\n",
      "   Changed_Credit_Limit  Num_Credit_Inquiries  Credit_Mix Outstanding_Debt  \\\n",
      "0                 11.27                   4.0           _              NaN   \n",
      "1                 11.27                   4.0         NaN           809.98   \n",
      "2                   NaN                   4.0        Good           809.98   \n",
      "3                  6.27                   4.0        Good           809.98   \n",
      "4                 11.27                   4.0        Good           809.98   \n",
      "\n",
      "  Credit_Utilization_Ratio     Credit_History_Age Payment_of_Min_Amount  \\\n",
      "0                26.822620  22 Years and 1 Months                    No   \n",
      "1                31.944960                    NaN                    No   \n",
      "2                28.609352                    NaN                    No   \n",
      "3                      NaN  22 Years and 4 Months                    No   \n",
      "4                24.797347  22 Years and 5 Months                    No   \n",
      "\n",
      "  Amount_invested_monthly  Credit_Score  \n",
      "0       80.41529543900253             3  \n",
      "1      118.28022162236736             3  \n",
      "2         81.699521264648             3  \n",
      "3       199.4580743910713             3  \n",
      "4      41.420153086217326             3  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "\n",
      "The dataset contains 27 columns\n",
      "The dataset contains 100000 rows\n"
     ]
    }
   ],
   "source": [
    "# Head of the DataFrame\n",
    "print(\"Head of the DataFrame:\")\n",
    "print(credit_card_df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Identify Number of Columns and Rows\n",
    "n_rows = len(credit_card_df)\n",
    "n_columns = len(credit_card_df.columns)\n",
    "print(f\"The dataset contains {n_columns} columns\")\n",
    "print(f\"The dataset contains {n_rows} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continued our first and superficial analysis of the dataset by retrieving the names of the columns and their data types. This is done using the function `.dtypes`, in order to understand the nature of the data we are dealing with and to check if there are any inconsistencies in the dataset (e.g. a column that should be numeric but is stored as a string). \n",
    "In addition, we can also check the number of missing values in each column using the function `.isnull().sum()`. This is useful to understand if we need to impute the dataset before proceeding with the analysis. \n",
    "Finally, we can also check the number of unique values in each column using the function `.nunique()`, which is relevant to discover the categorical variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Data Type  Unique Values  Missing Values\n",
      "ID                          object          90058            9942\n",
      "Customer_ID                 object          12500            9893\n",
      "Month                       object              8            9938\n",
      "Name                        object          10139           18887\n",
      "City                        object              4            9851\n",
      "Street                      object             31            9920\n",
      "Age                        float64           1501           14488\n",
      "SSN                         object          12501           10001\n",
      "Occupation                  object             21            9944\n",
      "Annual_Income               object          18438           10094\n",
      "Monthly_Inhand_Salary      float64          13204           23595\n",
      "Num_Bank_Accounts          float64            879            9833\n",
      "Num_Credit_Card            float64           1132           10062\n",
      "Interest_Rate              float64           1597            9849\n",
      "Num_of_Loan                 object            399           10191\n",
      "Type_of_Loan                object           6260           20312\n",
      "Delay_from_due_date        float64             73            9988\n",
      "Num_of_Delayed_Payment      object            684           16218\n",
      "Changed_Credit_Limit        object           4314           10067\n",
      "Num_Credit_Inquiries       float64           1124           11898\n",
      "Credit_Mix                  object              4            9915\n",
      "Outstanding_Debt            object          13088            9963\n",
      "Credit_Utilization_Ratio   float64          90025            9975\n",
      "Credit_History_Age          object            404           18209\n",
      "Payment_of_Min_Amount       object              3            9957\n",
      "Amount_invested_monthly     object          81866           14120\n",
      "Credit_Score                 int64              4               0\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with column data types and number of unique values\n",
    "column_summary = pd.DataFrame({\n",
    "    \"Data Type\": credit_card_df.dtypes,\n",
    "    \"Unique Values\": credit_card_df.nunique(),\n",
    "    \"Missing Values\": credit_card_df.isnull().sum(),\n",
    "})\n",
    "\n",
    "# Display the combined summary\n",
    "print(column_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many columns, such as **Annual Income**, are classified as categorical. Upon inspecting the dataset, we observed that several of these columns contain special characters like `\"_ \\@ $\"`, which cause the values to be treated as objects instead of numbers. To address this, we replace these special characters with an empty string `''`. This process may introduce new missing values, but it is necessary for accurate analysis. A cell containing only special characters provides no meaningful information and can be reasonably treated as a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual_Income              float64\n",
      "Num_of_Loan                float64\n",
      "Num_of_Delayed_Payment     float64\n",
      "Changed_Credit_Limit       float64\n",
      "Outstanding_Debt           float64\n",
      "Amount_invested_monthly    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def simple_clean_numeric(df, cols):\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert specified columns to numeric by removing any non-digit characters \n",
    "    (except for period and minus sign) and then converting to numeric.\n",
    "    \n",
    "    Parameters:\n",
    "      df: pandas DataFrame\n",
    "      cols: List of column names to clean and convert\n",
    "      \n",
    "    Returns:\n",
    "      A new DataFrame with the specified columns cleaned and converted to numeric.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    for col in cols:\n",
    "        if col in df_clean.columns:\n",
    "            # Convert to string and remove all characters except digits, period, and minus sign.\n",
    "            cleaned = df_clean[col].astype(str).str.replace(r'[^0-9\\.-]', '', regex=True)\n",
    "            # Convert the cleaned string to numeric (float); unconvertible values become NaN.\n",
    "            df_clean[col] = pd.to_numeric(cleaned, errors='coerce')\n",
    "    return df_clean\n",
    "\n",
    "# List of columns that should be numeric\n",
    "should_be_numeric = [\n",
    "    'Annual_Income', \n",
    "    'Num_of_Loan', \n",
    "    'Num_of_Delayed_Payment', \n",
    "    'Changed_Credit_Limit', \n",
    "    'Outstanding_Debt', \n",
    "    'Amount_invested_monthly'\n",
    "]\n",
    "\n",
    "# Clean and convert the data\n",
    "credit_card_df = simple_clean_numeric(credit_card_df, should_be_numeric)\n",
    "\n",
    "# Verify that the columns are now numeric\n",
    "print(credit_card_df[should_be_numeric].dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting the columns to numeric, we can correctly check the data types of the columns again using the function `.dtypes`. This allows us to verify that the columns are now in the appropriate format for analysis, as well as to split the dataset into categorical and numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15 numerical features:\n",
      "['Age', 'Annual_Income', 'Monthly_Inhand_Salary', 'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate', 'Num_of_Loan', 'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Amount_invested_monthly', 'Credit_Score']\n",
      "\n",
      "\n",
      "There are 12 categorical features:\n",
      "['ID', 'Customer_ID', 'Month', 'Name', 'City', 'Street', 'SSN', 'Occupation', 'Type_of_Loan', 'Credit_Mix', 'Credit_History_Age', 'Payment_of_Min_Amount']\n"
     ]
    }
   ],
   "source": [
    "##Selecting numerical features\n",
    "numerical_data = credit_card_df.select_dtypes(include='number')\n",
    "#append the features of numerical_data to list\n",
    "numerical_features = numerical_data.columns.tolist()\n",
    "\n",
    "#Selecting categoricalfeatures\n",
    "categorical_data = credit_card_df.select_dtypes(include= 'object')\n",
    "#append the features of categorical_data to list\n",
    "categorical_features = categorical_data.columns.tolist()\n",
    "\n",
    "print(f'There are {len(numerical_features)} numerical features:')\n",
    "print(numerical_features)\n",
    "print('\\n')\n",
    "print(f'There are {len(categorical_features)} categorical features:')\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the numerical features in the correct format, we can proceed with a further analysis of the dataset. We can check the summary statistics of the dataset using the function `.describe()`, which provides useful information such as the mean, standard deviation, minimum and maximum values, and quartiles for each numerical column. This is useful to understand the distribution of the data and to identify any potential outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "std",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "25%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "50%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "75%",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "bf84a1a8-88ff-4422-90db-492710ada17e",
       "rows": [
        [
         "Age",
         "85512.0",
         "110.2268453550379",
         "684.9075881251607",
         "-500.0",
         "24.0",
         "33.0",
         "42.0",
         "8698.0"
        ],
        [
         "Annual_Income",
         "89906.0",
         "176544.7955903944",
         "1429012.6532640015",
         "7005.93",
         "19453.05",
         "37579.75",
         "72760.04",
         "24198062.0"
        ],
        [
         "Monthly_Inhand_Salary",
         "76405.0",
         "4185.789272430268",
         "3178.5605057874163",
         "303.6454166666666",
         "1625.02375",
         "3086.683333333333",
         "5940.3175",
         "15204.633333333331"
        ],
        [
         "Num_Bank_Accounts",
         "90167.0",
         "17.10517151507758",
         "117.72821482138556",
         "-1.0",
         "3.0",
         "6.0",
         "7.0",
         "1798.0"
        ],
        [
         "Num_Credit_Card",
         "89938.0",
         "22.449409593275366",
         "129.03723280868533",
         "0.0",
         "4.0",
         "5.0",
         "7.0",
         "1499.0"
        ],
        [
         "Interest_Rate",
         "90151.0",
         "72.3439229736775",
         "465.6848217492534",
         "1.0",
         "8.0",
         "13.0",
         "20.0",
         "5797.0"
        ],
        [
         "Num_of_Loan",
         "89809.0",
         "3.0651159683327953",
         "63.193373111040536",
         "-100.0",
         "1.0",
         "3.0",
         "5.0",
         "1496.0"
        ],
        [
         "Delay_from_due_date",
         "90012.0",
         "21.081155845887213",
         "14.862706593658286",
         "-5.0",
         "10.0",
         "18.0",
         "28.0",
         "67.0"
        ],
        [
         "Num_of_Delayed_Payment",
         "83782.0",
         "31.041369267861832",
         "226.37029623645358",
         "-3.0",
         "9.0",
         "14.0",
         "18.0",
         "4388.0"
        ],
        [
         "Changed_Credit_Limit",
         "88058.0",
         "10.389097185945625",
         "6.78282852046774",
         "-6.49",
         "5.34",
         "9.41",
         "14.86",
         "36.97"
        ],
        [
         "Num_Credit_Inquiries",
         "88102.0",
         "27.84688202310958",
         "193.61084320625267",
         "0.0",
         "3.0",
         "6.0",
         "9.0",
         "2597.0"
        ],
        [
         "Outstanding_Debt",
         "90037.0",
         "1426.10076368604",
         "1154.6586736115098",
         "0.23",
         "566.4",
         "1167.04",
         "1943.08",
         "4998.07"
        ],
        [
         "Credit_Utilization_Ratio",
         "90025.0",
         "32.2909672206417",
         "5.1146698214563395",
         "20.10076996070649",
         "28.064417788600345",
         "32.31827997860508",
         "36.50227175030202",
         "50.00000000000001"
        ],
        [
         "Amount_invested_monthly",
         "85880.0",
         "636.1466616039725",
         "2041.2635191122513",
         "0.0",
         "74.3988463596237",
         "135.6726007059409",
         "265.1985959322373",
         "10000.0"
        ],
        [
         "Credit_Score",
         "100000.0",
         "1.88064",
         "0.6830650453496812",
         "0.0",
         "1.0",
         "2.0",
         "2.0",
         "3.0"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>85512.0</td>\n",
       "      <td>110.226845</td>\n",
       "      <td>6.849076e+02</td>\n",
       "      <td>-500.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>8.698000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annual_Income</th>\n",
       "      <td>89906.0</td>\n",
       "      <td>176544.795590</td>\n",
       "      <td>1.429013e+06</td>\n",
       "      <td>7005.930000</td>\n",
       "      <td>19453.050000</td>\n",
       "      <td>37579.750000</td>\n",
       "      <td>72760.040000</td>\n",
       "      <td>2.419806e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Monthly_Inhand_Salary</th>\n",
       "      <td>76405.0</td>\n",
       "      <td>4185.789272</td>\n",
       "      <td>3.178561e+03</td>\n",
       "      <td>303.645417</td>\n",
       "      <td>1625.023750</td>\n",
       "      <td>3086.683333</td>\n",
       "      <td>5940.317500</td>\n",
       "      <td>1.520463e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_Bank_Accounts</th>\n",
       "      <td>90167.0</td>\n",
       "      <td>17.105172</td>\n",
       "      <td>1.177282e+02</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.798000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_Credit_Card</th>\n",
       "      <td>89938.0</td>\n",
       "      <td>22.449410</td>\n",
       "      <td>1.290372e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.499000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interest_Rate</th>\n",
       "      <td>90151.0</td>\n",
       "      <td>72.343923</td>\n",
       "      <td>4.656848e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>5.797000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_of_Loan</th>\n",
       "      <td>89809.0</td>\n",
       "      <td>3.065116</td>\n",
       "      <td>6.319337e+01</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.496000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delay_from_due_date</th>\n",
       "      <td>90012.0</td>\n",
       "      <td>21.081156</td>\n",
       "      <td>1.486271e+01</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>6.700000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_of_Delayed_Payment</th>\n",
       "      <td>83782.0</td>\n",
       "      <td>31.041369</td>\n",
       "      <td>2.263703e+02</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.388000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Changed_Credit_Limit</th>\n",
       "      <td>88058.0</td>\n",
       "      <td>10.389097</td>\n",
       "      <td>6.782829e+00</td>\n",
       "      <td>-6.490000</td>\n",
       "      <td>5.340000</td>\n",
       "      <td>9.410000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>3.697000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Num_Credit_Inquiries</th>\n",
       "      <td>88102.0</td>\n",
       "      <td>27.846882</td>\n",
       "      <td>1.936108e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.597000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outstanding_Debt</th>\n",
       "      <td>90037.0</td>\n",
       "      <td>1426.100764</td>\n",
       "      <td>1.154659e+03</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>566.400000</td>\n",
       "      <td>1167.040000</td>\n",
       "      <td>1943.080000</td>\n",
       "      <td>4.998070e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_Utilization_Ratio</th>\n",
       "      <td>90025.0</td>\n",
       "      <td>32.290967</td>\n",
       "      <td>5.114670e+00</td>\n",
       "      <td>20.100770</td>\n",
       "      <td>28.064418</td>\n",
       "      <td>32.318280</td>\n",
       "      <td>36.502272</td>\n",
       "      <td>5.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amount_invested_monthly</th>\n",
       "      <td>85880.0</td>\n",
       "      <td>636.146662</td>\n",
       "      <td>2.041264e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>74.398846</td>\n",
       "      <td>135.672601</td>\n",
       "      <td>265.198596</td>\n",
       "      <td>1.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_Score</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>1.880640</td>\n",
       "      <td>6.830650e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             count           mean           std          min  \\\n",
       "Age                        85512.0     110.226845  6.849076e+02  -500.000000   \n",
       "Annual_Income              89906.0  176544.795590  1.429013e+06  7005.930000   \n",
       "Monthly_Inhand_Salary      76405.0    4185.789272  3.178561e+03   303.645417   \n",
       "Num_Bank_Accounts          90167.0      17.105172  1.177282e+02    -1.000000   \n",
       "Num_Credit_Card            89938.0      22.449410  1.290372e+02     0.000000   \n",
       "Interest_Rate              90151.0      72.343923  4.656848e+02     1.000000   \n",
       "Num_of_Loan                89809.0       3.065116  6.319337e+01  -100.000000   \n",
       "Delay_from_due_date        90012.0      21.081156  1.486271e+01    -5.000000   \n",
       "Num_of_Delayed_Payment     83782.0      31.041369  2.263703e+02    -3.000000   \n",
       "Changed_Credit_Limit       88058.0      10.389097  6.782829e+00    -6.490000   \n",
       "Num_Credit_Inquiries       88102.0      27.846882  1.936108e+02     0.000000   \n",
       "Outstanding_Debt           90037.0    1426.100764  1.154659e+03     0.230000   \n",
       "Credit_Utilization_Ratio   90025.0      32.290967  5.114670e+00    20.100770   \n",
       "Amount_invested_monthly    85880.0     636.146662  2.041264e+03     0.000000   \n",
       "Credit_Score              100000.0       1.880640  6.830650e-01     0.000000   \n",
       "\n",
       "                                   25%           50%           75%  \\\n",
       "Age                          24.000000     33.000000     42.000000   \n",
       "Annual_Income             19453.050000  37579.750000  72760.040000   \n",
       "Monthly_Inhand_Salary      1625.023750   3086.683333   5940.317500   \n",
       "Num_Bank_Accounts             3.000000      6.000000      7.000000   \n",
       "Num_Credit_Card               4.000000      5.000000      7.000000   \n",
       "Interest_Rate                 8.000000     13.000000     20.000000   \n",
       "Num_of_Loan                   1.000000      3.000000      5.000000   \n",
       "Delay_from_due_date          10.000000     18.000000     28.000000   \n",
       "Num_of_Delayed_Payment        9.000000     14.000000     18.000000   \n",
       "Changed_Credit_Limit          5.340000      9.410000     14.860000   \n",
       "Num_Credit_Inquiries          3.000000      6.000000      9.000000   \n",
       "Outstanding_Debt            566.400000   1167.040000   1943.080000   \n",
       "Credit_Utilization_Ratio     28.064418     32.318280     36.502272   \n",
       "Amount_invested_monthly      74.398846    135.672601    265.198596   \n",
       "Credit_Score                  1.000000      2.000000      2.000000   \n",
       "\n",
       "                                   max  \n",
       "Age                       8.698000e+03  \n",
       "Annual_Income             2.419806e+07  \n",
       "Monthly_Inhand_Salary     1.520463e+04  \n",
       "Num_Bank_Accounts         1.798000e+03  \n",
       "Num_Credit_Card           1.499000e+03  \n",
       "Interest_Rate             5.797000e+03  \n",
       "Num_of_Loan               1.496000e+03  \n",
       "Delay_from_due_date       6.700000e+01  \n",
       "Num_of_Delayed_Payment    4.388000e+03  \n",
       "Changed_Credit_Limit      3.697000e+01  \n",
       "Num_Credit_Inquiries      2.597000e+03  \n",
       "Outstanding_Debt          4.998070e+03  \n",
       "Credit_Utilization_Ratio  5.000000e+01  \n",
       "Amount_invested_monthly   1.000000e+04  \n",
       "Credit_Score              3.000000e+00  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_card_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instantly see that the dataset contains some strange values. As an example, the column **Age** has both negative (minimun value = -500) and very high values (maximum value = 8698). \n",
    "It is also possible to note that the target variable **Credit Score** score is more a categorical variable than a continuous one, as it has only 4 unique values. As a consequence, we will convert the target variable into a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "unique",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "top",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "freq",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b98462c3-06ba-4225-bf31-0d6c81fd1b71",
       "rows": [
        [
         "ID",
         "90058",
         "90058",
         "0x25fed",
         "1"
        ],
        [
         "Customer_ID",
         "90107",
         "12500",
         "CUS_0x8600",
         "8"
        ],
        [
         "Month",
         "90062",
         "8",
         "April",
         "11319"
        ],
        [
         "Name",
         "81113",
         "10139",
         "Langep",
         "42"
        ],
        [
         "City",
         "90149",
         "4",
         "Standhampton",
         "47744"
        ],
        [
         "Street",
         "90080",
         "31",
         "Quality Street",
         "5343"
        ],
        [
         "SSN",
         "89999",
         "12501",
         "#F%$D@*&8",
         "5008"
        ],
        [
         "Occupation",
         "90056",
         "21",
         "Journalist",
         "17002"
        ],
        [
         "Type_of_Loan",
         "79688",
         "6260",
         "Not Specified",
         "1272"
        ],
        [
         "Credit_Mix",
         "90085",
         "4",
         "Standard",
         "32875"
        ],
        [
         "Credit_History_Age",
         "81791",
         "404",
         "17 Years and 11 Months",
         "407"
        ],
        [
         "Payment_of_Min_Amount",
         "90043",
         "3",
         "Yes",
         "47204"
        ],
        [
         "Credit_Score",
         "100000",
         "4",
         "2",
         "52974"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 13
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>90058</td>\n",
       "      <td>90058</td>\n",
       "      <td>0x25fed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer_ID</th>\n",
       "      <td>90107</td>\n",
       "      <td>12500</td>\n",
       "      <td>CUS_0x8600</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month</th>\n",
       "      <td>90062</td>\n",
       "      <td>8</td>\n",
       "      <td>April</td>\n",
       "      <td>11319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>81113</td>\n",
       "      <td>10139</td>\n",
       "      <td>Langep</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>90149</td>\n",
       "      <td>4</td>\n",
       "      <td>Standhampton</td>\n",
       "      <td>47744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Street</th>\n",
       "      <td>90080</td>\n",
       "      <td>31</td>\n",
       "      <td>Quality Street</td>\n",
       "      <td>5343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSN</th>\n",
       "      <td>89999</td>\n",
       "      <td>12501</td>\n",
       "      <td>#F%$D@*&amp;8</td>\n",
       "      <td>5008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Occupation</th>\n",
       "      <td>90056</td>\n",
       "      <td>21</td>\n",
       "      <td>Journalist</td>\n",
       "      <td>17002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type_of_Loan</th>\n",
       "      <td>79688</td>\n",
       "      <td>6260</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>1272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_Mix</th>\n",
       "      <td>90085</td>\n",
       "      <td>4</td>\n",
       "      <td>Standard</td>\n",
       "      <td>32875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_History_Age</th>\n",
       "      <td>81791</td>\n",
       "      <td>404</td>\n",
       "      <td>17 Years and 11 Months</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payment_of_Min_Amount</th>\n",
       "      <td>90043</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>47204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_Score</th>\n",
       "      <td>100000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>52974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count unique                     top   freq\n",
       "ID                      90058  90058                 0x25fed      1\n",
       "Customer_ID             90107  12500              CUS_0x8600      8\n",
       "Month                   90062      8                   April  11319\n",
       "Name                    81113  10139                  Langep     42\n",
       "City                    90149      4            Standhampton  47744\n",
       "Street                  90080     31          Quality Street   5343\n",
       "SSN                     89999  12501               #F%$D@*&8   5008\n",
       "Occupation              90056     21              Journalist  17002\n",
       "Type_of_Loan            79688   6260           Not Specified   1272\n",
       "Credit_Mix              90085      4                Standard  32875\n",
       "Credit_History_Age      81791    404  17 Years and 11 Months    407\n",
       "Payment_of_Min_Amount   90043      3                     Yes  47204\n",
       "Credit_Score           100000      4                       2  52974"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert target variable to object type\n",
    "credit_card_df['Credit_Score'] = credit_card_df['Credit_Score'].astype('object')\n",
    "\n",
    "# Display the summary statistics for categorical features\n",
    "credit_card_df.describe(include=\"object\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly tried to plot the distribution of numerical variables without addressing outliers or noisy values. However, the presence of outliers and extreme values in the dataset made it difficult to visualize the distribution effectively. \n",
    "To improve the visualization, we decided to remove outliers from the dataset. However, because of the presence of an high number of missing values, we had to address them first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Missing values and imputation\n",
    "\n",
    "The dataset contains numerous missing values, likely due to poor data collection or entry errors. It is also possible that an event occurred, leading to the loss of many values. However, we observe that there are only 12,500 unique `Customer_ID` values across 100,000 entries. This indicates that many customers are repeated throughout the dataset. Consequently, for missing values that should remain consistent for the same customer, we can impute them using the corresponding values from other rows associated with the same customer.\n",
    "\n",
    "The first issue to fix is hence to address the missing values in the `Customer_ID` column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1) Imputation of missing values in the `Customer_ID` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly display the number of missing values in the `Customer_ID` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Customer_ID: 9893\n"
     ]
    }
   ],
   "source": [
    "missing_customerID = credit_card_df['Customer_ID'].isnull().sum()\n",
    "print(f\"Missing Customer_ID: {missing_customerID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times Customer ID is the same in 8 rows: 12500\n",
      "Number of unique Customer IDs: 12500\n"
     ]
    }
   ],
   "source": [
    "# Check if every 8 rows, the Customer ID is unique\n",
    "counter = 0\n",
    "for i in range(0, len(credit_card_df), 8): # Iterate through the DataFrame in steps of 8\n",
    "    if credit_card_df['Customer_ID'][i:i+7].nunique() > 1: # Check for 8 rows\n",
    "        print(f\"Different Customer ID found in rows {i} to {i+7}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "print(f\"Number of times Customer ID is the same in 8 rows: {counter}\")\n",
    "print(f\"Number of unique Customer IDs: {credit_card_df['Customer_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the output of the previous cell shows that the column `Customer_ID` has 12,500 unique values, and each of them is repeated 8 times for the 8 months in which the customer was monitored.\n",
    "This also implies that there is no customer with a missing `Customer_ID` value, as the number of unique values every 8 rows (which is confirmed to be 1) is equal to the number of unique values in the entire column. As a consequence, there are not wrong enntries, but only missing values, that can be easily imputed by taking the unique value of the `Customer_ID` column for each group of 8 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Customer_ID after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "seen_customer_ids = set()\n",
    "\n",
    "for i in range(0, len(credit_card_df)):\n",
    "    customer_id = credit_card_df['Customer_ID'][i]\n",
    "    if customer_id in seen_customer_ids or pd.isnull(customer_id):\n",
    "        # If the customer ID is already seen or is null we skip this row\n",
    "        continue\n",
    "    else:\n",
    "        # If the customer ID is not seen, we add it to the set\n",
    "        seen_customer_ids.add(customer_id)\n",
    "        # We use this customer ID to impute the missing values in the 8 rows of the group\n",
    "        credit_card_df.loc[(i//8*8) : (i//8*8) + 7, 'Customer_ID'] = customer_id\n",
    "\n",
    "# Check if there are still any missing values in the Customer_ID column\n",
    "missing_customerID = credit_card_df['Customer_ID'].isnull().sum()\n",
    "print(f\"Missing Customer_ID after imputation: {missing_customerID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times Customer ID is the same in 8 rows: 12500\n"
     ]
    }
   ],
   "source": [
    "# Check that there is still only one unique Customer_ID in every 8 rows\n",
    "counter = 0\n",
    "for i in range(0, len(credit_card_df), 8): # Iterate through the DataFrame in steps of 8\n",
    "    if credit_card_df['Customer_ID'][i:i+7].nunique() > 1: # Check for 8 rows\n",
    "        print(f\"Different Customer ID found in rows {i} to {i+7}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "print(f\"Number of times Customer ID is the same in 8 rows: {counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to fill missing Customer_ID values based on matching names\n",
    "Previous version, without the theory of sequential dataset\n",
    "```python\n",
    "\n",
    "def fill_missing_customer_id(df):\n",
    "    \"\"\"\n",
    "    Fill missing Customer_ID values using the most common ID for each name.\n",
    "    \n",
    "    Parameters:\n",
    "        df: The input DataFrame with Name and Customer_ID columns\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with missing Customer_ID values filled\n",
    "    \"\"\"\n",
    "    # Count missing values before\n",
    "    missing_before = df['Customer_ID'].isnull().sum()\n",
    "    print(f\"Missing Customer_ID values before: {missing_before}\")\n",
    "    \n",
    "    # Check if there are any missing values to fill\n",
    "    if missing_before == 0:\n",
    "        print(\"No missing Customer_ID values to fill.\")\n",
    "        return df\n",
    "    \n",
    "    # Create a copy to avoid modifying the original during calculation\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Only use rows that have both a Name and a valid Customer_ID to compute the mode\n",
    "    valid = df.dropna(subset=['Name', 'Customer_ID'])\n",
    "    \n",
    "    # Find the most common Customer_ID for each Name\n",
    "    mode_mapping = valid.groupby('Name')['Customer_ID'].agg(lambda x: x.mode().iloc[0] if not x.empty else None)\n",
    "    \n",
    "    # Create a mask for rows with missing Customer_ID\n",
    "    missing_mask = result_df['Customer_ID'].isnull()\n",
    "    \n",
    "    # Apply the mapping in one vectorized operation\n",
    "    result_df.loc[missing_mask, 'Customer_ID'] = result_df.loc[missing_mask, 'Name'].map(mode_mapping)\n",
    "    \n",
    "    # Count missing values after filling\n",
    "    missing_after = result_df['Customer_ID'].isnull().sum()\n",
    "    print(f\"Missing Customer_ID values after: {missing_after}\")\n",
    "    print(f\"Fixed {missing_before - missing_after} values ({(missing_before - missing_after)/missing_before:.2%} of missing values)\")\n",
    "    \n",
    "    # Check if any values couldn't be filled\n",
    "    if missing_after > 0:\n",
    "        print(f\"Warning: Could not fill {missing_after} Customer_ID values because their Names don't match any known Customer_ID\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Apply the function to fix missing Customer_ID values\n",
    "credit_card_df = fill_missing_customer_id(credit_card_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory of subsequential dataset\n",
    "\n",
    "We found out that each client has a monthly analysis of his credit cards on a range of months that goes from january to august. Here we want to prove that we can fix the remaining missing values for customer ID going in one of the other lines that are i range of january august and fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows per customer\n",
    "rows_per_customer = credit_card_df.groupby('Customer_ID').size()\n",
    "print(\"Average entries per customer:\", rows_per_customer.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we confirmed that each customer has an average of 8 rows (this is accurate since there are only 1891 missing Customer_IDs out of 100,000 entries). As the length of the range of months between January and August is 8, we can proceed to fix all the remaining missing Customer_IDs by first addressing the 'Month' column.\n",
    "\n",
    "Example of correct order:\n",
    "| Customer_ID | Month    |\n",
    "|-------------|----------|\n",
    "| Client 1    | January  |\n",
    "| Client 1    | February |\n",
    "| Client 1    | March    |\n",
    "| Client 1    | April    |\n",
    "| Client 1    | May      |\n",
    "| Client 1    | June     |\n",
    "| Client 1    | July     |\n",
    "| Client 1    | August   |\n",
    "\n",
    "Example of incorrect order:\n",
    "| Customer_ID | Month    |\n",
    "|-------------|----------|\n",
    "| Client 1    | January  |\n",
    "| Client 2    | January  |\n",
    "| Client 1    | February |\n",
    "| Client 3    | March    |\n",
    "| Client 1    | June     |\n",
    "| Client 5    | April    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the expected month order.\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August']\n",
    "\n",
    "# Reset the index to ensure we're working on the DataFrame's natural order (if needed)\n",
    "credit_card_df = credit_card_df.reset_index(drop=True)\n",
    "\n",
    "# Create a new month column with a repeating cycle: each group of 8 rows gets January to August.\n",
    "credit_card_df['Month'] = [month_order[i % 8] for i in range(len(credit_card_df))]\n",
    "\n",
    "# Check Missing Values in the Month Column\n",
    "missing_month = credit_card_df['Month'].isnull().sum()\n",
    "print(f\"Missing Month values: {missing_month}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the month order and a mapping to positions (0 for January, …, 7 for August).\n",
    "month_to_index = {month: i for i, month in enumerate(month_order)}\n",
    "\n",
    "def fill_missing_customer_id(df):\n",
    "    df = df.copy()\n",
    "    n_rows = len(df)\n",
    "    \n",
    "    # Iterate over each row where Customer_ID is missing.\n",
    "    for idx, row in df[df['Customer_ID'].isna()].iterrows():\n",
    "        # Determine the cycle position based on the assigned month.\n",
    "        # If for some reason month is missing, default to position 0.\n",
    "        m = row.get('Month', None)\n",
    "        m_index = month_to_index.get(m, 0)\n",
    "        \n",
    "        # Define the search window:\n",
    "        # Look m rows upward and (7 - m_index) rows downward.\n",
    "        window_start = max(idx - m_index, 0)\n",
    "        window_end = min(idx + (7 - m_index), n_rows - 1)\n",
    "        \n",
    "        # Extract the window of rows.\n",
    "        window = df.loc[window_start:window_end, 'Customer_ID']\n",
    "        # Get all non-missing Customer_ID values in that window.\n",
    "        non_missing_ids = window.dropna().unique()\n",
    "        \n",
    "        if len(non_missing_ids) == 1:\n",
    "            # If exactly one unique non-missing Customer_ID is found, assign it.\n",
    "            df.at[idx, 'Customer_ID'] = non_missing_ids[0]\n",
    "        elif len(non_missing_ids) > 1:\n",
    "            # If multiple candidate IDs are found, you could choose the first one,\n",
    "            # or implement more sophisticated logic (e.g. mode, or nearest by distance).\n",
    "            df.at[idx, 'Customer_ID'] = non_missing_ids[0]\n",
    "        else:\n",
    "            # If no candidate is found in the immediate window, you may decide to extend the search.\n",
    "            # For now, we'll leave it missing.\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "credit_card_df= fill_missing_customer_id(credit_card_df)\n",
    "\n",
    "# Check for any remaining missing values in Customer_ID\n",
    "print(f\"Missing Customer_ID after filling: {credit_card_df['Customer_ID'].isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now verify that the `Customer_ID` column is now filled with the correct values by checking the number of unique values each 8 rows. If there is a single unique value, we can be sure that the `Customer_ID` column is now filled with the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "unique_customers = set()\n",
    "for i in range(len(credit_card_df)):\n",
    "    if counter == 8:\n",
    "        counter = 0\n",
    "    if credit_card_df['Customer_ID'][i] not in unique_customers:\n",
    "        unique_customers.add((credit_card_df['Customer_ID'][i]))\n",
    "    elif credit_card_df['Customer_ID'][i] in unique_customers and counter == 0:\n",
    "        print(f\"Duplicate Customer_ID found at index {i}: {credit_card_df['Customer_ID'][i]}\")\n",
    "    \n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the columns 'Name', 'City', 'Street' and 'Occupation'\n",
    "\n",
    "For these columns we follow the principle we used before and we scan through the lines which have the same Customer ID to find the missing name city street and occupation that for sure won't change between January and August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_invariant = ['Name', 'City', 'Street', 'Occupation']\n",
    "for col in cols_invariant:\n",
    "    credit_card_df[col] = credit_card_df.groupby('Customer_ID')[col].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "# Check for any remaining missing values in the columns\n",
    "missing_values = credit_card_df[cols_invariant].isnull().sum()\n",
    "print(f\"Missing values in invariant columns after filling: {missing_values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Age\n",
    "\n",
    "### Fixing Age\n",
    "\n",
    "With the age column, we identified two main issues:\n",
    "\n",
    "1. **Inconsistent values across months:**  \n",
    "        For some customers, the age values fluctuate unrealistically between months. For example, in January, a customer might have an age of 23, in February -500 (which is impossible), and in March, it reverts back to 23. These anomalies will be addressed later, but for now, we need to replace such values with a reasonable temporary solution.\n",
    "\n",
    "2. **Age increment due to birthdates between January and August:**  \n",
    "        Many customers have their birthdays between January and August, causing their age to increase by one year during this period. For these cases, it is acceptable to have some rows with one age value (e.g., 54) and others with the incremented value (e.g., 55). \n",
    "\n",
    "To handle this, we substiute the missing values or the values that have a difference with the previous one of more than 1 with the value of the previous month. If the latter is not available, we can use the next month value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_age_column(df):\n",
    "    \"\"\"\n",
    "    For the same customer, if the value in the 'Age' column is greater than the subsequent\n",
    "    or precedent value by more than one, replace it with the precedent value if available,\n",
    "    otherwise use the subsequent value. Also, fill NaN values with the precedent value\n",
    "    if available, otherwise use the subsequent value.\n",
    "    \n",
    "    Parameters:\n",
    "        df: pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with cleaned 'Age' column\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Remove values wuth 'Age' greater than 100 and less than 0, put them to NaN\n",
    "    df.loc[(df['Age'] > 100) | (df['Age'] < 0), 'Age'] = np.nan\n",
    "\n",
    "    # If the value in the 'Age' column is greater than the subsequent or precedent value by more than one,\n",
    "    # replace it with the precedent value if available, otherwise use the subsequent value.\n",
    "\n",
    "    for idx in range(1, len(df)):\n",
    "        if pd.isna(df.at[idx, 'Age']):\n",
    "            # If the current value is NaN, check the precedent and subsequent values and fill accordingly\n",
    "            if idx > 0 and not pd.isna(df.at[idx - 1, 'Age']): # precedent value\n",
    "                df.at[idx, 'Age'] = df.at[idx - 1, 'Age']\n",
    "            elif idx < len(df) - 1 and not pd.isna(df.at[idx + 1, 'Age']): # subsequent value\n",
    "                df.at[idx, 'Age'] = df.at[idx + 1, 'Age']\n",
    "        else:\n",
    "            # If the current value is not NaN, check the precedent and subsequent values\n",
    "            if idx > 0 and abs(df.at[idx, 'Age'] - df.at[idx - 1, 'Age']) > 1:\n",
    "                df.at[idx, 'Age'] = df.at[idx - 1, 'Age']\n",
    "            if idx < len(df) - 1 and abs(df.at[idx, 'Age'] - df.at[idx + 1, 'Age']) > 1:\n",
    "                df.at[idx, 'Age'] = df.at[idx + 1, 'Age']\n",
    "\n",
    "    \n",
    "    # Fill any remaining NaN values with the precedent value if available, otherwise use the subsequent value\n",
    "    df['Age'] = df['Age'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    return df\n",
    "\n",
    "credit_card_df = clean_age_column(credit_card_df)\n",
    "\n",
    "\n",
    "# Check for any remaining missing values in the Age column\n",
    "print(f\"Missing Age values after filling: {credit_card_df['Age'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing SSN Values\n",
    "We have a column called SSN that represents the Social Security Number, which should uniquely identify a customer. In our dataset, each customer appears in 8 consecutive rows (one per month from January to August). Since a customer's SSN is expected to remain constant over time, any missing or invalid SSN in one row can be reliably filled using a valid SSN from another row belonging to the same customer.\n",
    "\n",
    "Our approach is as follows:\n",
    "1. **Clean the SSN values:**  \n",
    "   We define a function to verify if the SSN matches the standard format (three digits, a dash, two digits, a dash, and four digits). Values that do not meet this pattern or that correspond to known placeholders are set as missing (NaN).\n",
    "\n",
    "2. **Fill missing values:**  \n",
    "   By grouping the data using the Customer_ID, we can apply forward fill and backward fill within each group. This ensures that if at least one valid SSN exists among the 8 rows for the same customer, it is propagated to fill any missing values.\n",
    "\n",
    "This method leverages the consistency of SSN within each customer group and the sequential structure of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Convert special placeholders into NaN.\n",
    "def is_valid_ssn(x):\n",
    "    \"\"\"\n",
    "    Checks if x matches the typical SSN format: 3 digits, '-', 2 digits, '-', 4 digits.\n",
    "    Example: 821-00-0265\n",
    "    \"\"\"\n",
    "    pattern = r'^\\d{3}-\\d{2}-\\d{4}$'\n",
    "    return bool(re.match(pattern, str(x)))\n",
    "\n",
    "def clean_ssn(ssn_value):\n",
    "    \"\"\"\n",
    "    If ssn_value does not match the SSN pattern, return NaN.\n",
    "    \"\"\"\n",
    "    if ssn_value is None:\n",
    "        return np.nan\n",
    "    ssn_str = str(ssn_value)\n",
    "    # Treat certain special strings as missing values:\n",
    "    # e.g., #F%$D@*&8, missing, Missing value, etc.\n",
    "    placeholders = {\"#F%$D@*&8\", \"Missing value\", \"_\", \"\"}  # Example placeholders\n",
    "    if ssn_str in placeholders or not is_valid_ssn(ssn_str):\n",
    "        return np.nan\n",
    "    return ssn_str\n",
    "\n",
    "# Apply the cleaning function to the 'SSN' column\n",
    "credit_card_df['SSN'] = credit_card_df['SSN'].apply(clean_ssn)\n",
    "\n",
    "# 2) Group by Customer_ID and apply forward fill (ffill) and backward fill (bfill)\n",
    "credit_card_df['SSN'] = credit_card_df.groupby('Customer_ID')['SSN'].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "# 3) Check for any remaining missing values in the SSN column\n",
    "missing_ssn = credit_card_df['SSN'].isnull().sum()\n",
    "print(f\"Missing SSN values after filling: {missing_ssn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Column Consistency Across Customer Records\n",
    "We suspect that some columns (for example, Annual_Income, Monthly_Inhand_Salary, Num_Bank_Accounts, Num_Credit_Card, Interest_Rate, and Num_of_Loan) should remain constant over the 8 monthly records for each customer. However, there is a possibility that for some columns (such as Monthly_Inhand_Salary) the values might change from month to month. \n",
    "\n",
    "To decide if we can impute missing values using the same method as before (forward fill and backward fill based on Customer_ID), we first need to verify that the values in these columns do not vary within the same customer. The code below groups the data by Customer_ID and computes the number of unique values for each candidate column obviously excluding the missing values. This allows us to calculate the percentage of customers that have more than one unique value per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the candidate columns that might be invariant across months for each customer\n",
    "candidate_cols = [\n",
    "    \"Annual_Income\",\n",
    "    \"Monthly_Inhand_Salary\",\n",
    "    \"Num_Bank_Accounts\",\n",
    "    \"Num_Credit_Card\",\n",
    "    \"Interest_Rate\",\n",
    "    \"Num_of_Loan\"\n",
    "]\n",
    "\n",
    "consistency_results = {}\n",
    "\n",
    "# Group the dataframe by Customer_ID and calculate the number of unique non-missing values for each column in each group\n",
    "for col in candidate_cols:\n",
    "    group_nunique = credit_card_df.groupby(\"Customer_ID\")[col].nunique(dropna=True)  # ignore NaN\n",
    "    customers_with_variation = (group_nunique > 1).sum()\n",
    "    total_customers = group_nunique.shape[0]\n",
    "    proportion_with_variation = customers_with_variation / total_customers\n",
    "    consistency_results[col] = proportion_with_variation\n",
    "\n",
    "print(\"Proportion of customers with variation in each column (ignoring NaNs):\")\n",
    "for column, prop_var in consistency_results.items():\n",
    "    print(f\"{column}: {prop_var:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Values for Columns with Low Variation\n",
    "\n",
    "Based on the consistency check, some columns (for example, Annual_Income, Monthly_Inhand_Salary, and Num_Bank_Accounts) show variation in only around 15–18% of customers. For these columns, we can assume that they are mostly invariant per customer. Therefore, we can impute missing values by grouping by Customer_ID and using forward fill followed by backward fill to propagate the available value.\n",
    "\n",
    "For columns that exceed a chosen variation threshold (e.g., more than 20% variation), a different imputation strategy will be necessary, but for now we focus on the ones that appear to be invariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a threshold for variation (here we use 20%)\n",
    "variation_threshold = 0.20\n",
    "\n",
    "# Identify columns that are considered invariant (low variation)\n",
    "invariant_cols = [col for col, prop in consistency_results.items() if prop < variation_threshold]\n",
    "\n",
    "print(\"Columns assumed to be invariant:\", invariant_cols)\n",
    "\n",
    "# Impute missing values for these invariant columns using forward fill and backward fill by Customer_ID\n",
    "for col in invariant_cols:\n",
    "    # Drop impossible values (e.g., negative values for Annual_Income)\n",
    "    if col == 'Annual_Income':\n",
    "        credit_card_df[col] = credit_card_df[col].where(credit_card_df[col] >= 0, np.nan)\n",
    "    elif col == \"Num_Bank_Accounts\":\n",
    "        credit_card_df[col] = credit_card_df[col].where((credit_card_df[col] >= 0) & (credit_card_df[col] <= 100), np.nan)\n",
    "    \n",
    "    \n",
    "    # Group by Customer_ID and propagate non-missing values within each group\n",
    "    credit_card_df[col] = credit_card_df.groupby('Customer_ID')[col].transform(lambda x: x.ffill().bfill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for columns which have more than 20% \n",
    "\n",
    "After analyzing our dataset and examining the distributions of various numeric columns, we noticed that for **Interest_Rate**, **Num_Credit_Card**, and **Num_of_Loan** the distributions (when excluding extreme outliers) are very tight and consistent within each customer. Although we observe some variation, most of it is due to outlier values or data entry errors. Therefore, these columns should ideally be invariant for each customer. \n",
    "\n",
    "Based on the distribution analysis and summary statistics, we concluded that imputing missing values with the mode (i.e., the most common valid value) for each customer is the best approach. This method leverages the fact that, apart from a few anomalous outliers, the true value remains constant over time. The code below fills in the missing values for these three columns by grouping the data by `Customer_ID` and replacing missing values with the mode of the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for the selected columns using the mode within each Customer_ID group\n",
    "columns_to_impute = [\"Interest_Rate\", \"Num_Credit_Card\", \"Num_of_Loan\"]\n",
    "\n",
    "# For each of these columns, fill missing values with the mode of the values within each customer group\n",
    "for col in columns_to_impute:\n",
    "    # Drop impossible values (e.g., negative values for Interest_Rate)\n",
    "    if col == 'Interest_Rate':\n",
    "        credit_card_df[col] = credit_card_df[col].where(credit_card_df[col] >= 0, np.nan)\n",
    "    elif col == \"Num_Credit_Card\":\n",
    "        credit_card_df[col] = credit_card_df[col].where((credit_card_df[col] >= 0) & (credit_card_df[col] <= 100), np.nan)\n",
    "    elif col == \"Num_of_Loan\":\n",
    "        credit_card_df[col] = credit_card_df[col].where((credit_card_df[col] >= 0) & (credit_card_df[col] <= 100), np.nan)\n",
    "    credit_card_df[col] = credit_card_df.groupby(\"Customer_ID\")[col].transform(lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else np.nan))\n",
    "\n",
    "credit_card_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the 'Type_of_Loan' Missing Values\n",
    "\n",
    "For the 'Type_of_Loan' column, we noticed that the value is the same for all records of a given customer or not specified. We therefore impute missing values by grouping the dataset by `Customer_ID` and replacing missing entries with the most frequent (mode) value in that group. However, if all the records for a customer are missing, we fill those missing values with the string \"Not Specified\", which is appropriate for a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique values for the columns type of loan\n",
    "unique_loans = credit_card_df['Type_of_Loan'].unique()\n",
    "print(\"Unique values in 'Type_of_Loan':\", unique_loans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_type_of_loan(group):\n",
    "    # Remove missing values from the group\n",
    "    valid = group.dropna()\n",
    "    # If the group is completely missing values, fill all with 'Not Specified'\n",
    "    if valid.empty:\n",
    "        return group.fillna(\"Not Specified\")\n",
    "    else:\n",
    "        # Otherwise, compute the mode (most frequent value)\n",
    "        mode_val = valid.mode().iloc[0]\n",
    "        # Fill missing values with the computed mode\n",
    "        return group.fillna(mode_val)\n",
    "\n",
    "# Apply the function on the 'Type_of_Loan' column grouped by Customer_ID\n",
    "credit_card_df['Type_of_Loan'] = credit_card_df.groupby(\"Customer_ID\")['Type_of_Loan'].transform(fill_type_of_loan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this analysis we noticed that in the next columns of the dataset we have some columns that follow the path and others that clearly changes entries between each row of the same customer. For those we'll proceed later. The columns that we can fill as we did so far are: 'Num_Credit_Inquiries', 'Outstanding_Debt', 'Credit_History_Age'. For credit history age we have this format for a client:\n",
    "\n",
    "| Customer_ID | Credit_History_Age|\n",
    "|-------------|----------|\n",
    "| Client 1    | 22y and 1m |\n",
    "| Client 1    | 22y and 2m |\n",
    "| Client 1    | 22y and 3m |\n",
    "| Client 1    | 22y and 4m |\n",
    "| Client 1    | 22y and 5m |\n",
    "| Client 1    | 22y and 6m |\n",
    "| Client 1    | 22y and 7m |\n",
    "| Client 1    | 22y and 8m |\n",
    "\n",
    "As the year is the same and the month change, we can use the customer ID and the month together to fill the missing values for this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for 'Num_Credit_Inquiries' and 'Outstanding_Debt'\n",
    "for col in [\"Num_Credit_Inquiries\", \"Outstanding_Debt\"]:\n",
    "    credit_card_df[col] = credit_card_df.groupby(\"Customer_ID\")[col].transform(lambda x: x.ffill().bfill())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plan is to impute any missing values in 'Credit_History_Age' by:\n",
    "1. Determining the common year for that customer using any non-missing entry.\n",
    "2. Retrieving the month for the current row from the 'Month' column (by mapping the month name to a number from 1 to 8).\n",
    "3. Constructing a value in the format: \"YY Years and X Months\", where \"YY\" is the common year and \"X\" is the mapped month number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to convert the string format into total months\n",
    "def convert_to_months(ch_str):\n",
    "    \"\"\"\n",
    "    Converts a string like '22 Years and 4 Months'\n",
    "    to an integer representing the total number of months.\n",
    "    \"\"\"\n",
    "    if pd.isna(ch_str):\n",
    "        return np.nan  # return NaN if the value is missing\n",
    "    # Use regex to extract the numbers for years and months.\n",
    "    match = re.search(r'(\\d+)\\s*Years?\\s*and\\s*(\\d+)\\s*Months?', ch_str)\n",
    "    if match:\n",
    "        years = int(match.group(1))\n",
    "        months = int(match.group(2))\n",
    "        return years * 12 + months\n",
    "    return np.nan\n",
    "\n",
    "# Define a helper function to convert total months back to the formatted string\n",
    "def convert_from_months(total_months):\n",
    "    \"\"\"\n",
    "    Converts a total number of months into a string in the format:\n",
    "    '<years> Years and <months> Months'\n",
    "    \"\"\"\n",
    "    if pd.isna(total_months):\n",
    "        return np.nan\n",
    "    years = int(total_months) // 12\n",
    "    months = int(total_months) % 12\n",
    "    return f\"{years} Years and {months} Months\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the total months representation\n",
    "credit_card_df['Credit_History_Age'] = credit_card_df['Credit_History_Age'].apply(convert_to_months)\n",
    "\n",
    "# Perform linear interpolation on the numeric column.\n",
    "# This will fill missing values by assuming the pattern is linear (one month increment as in your example).\n",
    "credit_card_df['Credit_History_Age'] = credit_card_df['Credit_History_Age'].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# Convert the interpolated numeric values back to the original string format\n",
    "credit_card_df['Credit_History_Age'] = credit_card_df['Credit_History_Age'].apply(convert_from_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We used the Interquartile Range (IQR) method to identify and remove these outliers. The IQR is a measure of statistical dispersion and is calculated as the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of the data. Any data point that falls below $Q1 - 1.5 \\times IQR$ or above $Q3 + 1.5 \\times IQR$ is considered an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing outliers from the dataset\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    \"\"\"\n",
    "    Remove outliers from a specified column in a DataFrame using the IQR method.\n",
    "    \n",
    "    Parameters:\n",
    "      df: pandas DataFrame\n",
    "      column: Column name from which to remove outliers\n",
    "      \n",
    "    Returns:\n",
    "      A new DataFrame with outliers removed from the specified column.\n",
    "    \"\"\"\n",
    "    \n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 10 * IQR\n",
    "    upper_bound = Q3 + 10 * IQR\n",
    "    \n",
    "    # Filter the DataFrame to remove outliers\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "print(f\"Shape of the DataFrame before removing outliers: {credit_card_df.shape}\")\n",
    "\n",
    "# Loop through each numerical feature and remove outliers\n",
    "for col in numerical_features:\n",
    "    credit_card_df = remove_outliers(credit_card_df, col)\n",
    "\n",
    "# Display the shape of the DataFrame after removing outliers\n",
    "print(f\"Shape of the DataFrame after removing outliers: {credit_card_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of credit card score respect to a normal distribution to see i there are skewnesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# Get the list of numerical feature names from the cleaned dataframe.\n",
    "numeric_features = credit_card_df.select_dtypes(include=['number']).columns\n",
    "n_features = len(numeric_features)\n",
    "\n",
    "# Define grid size\n",
    "n_cols = 3  # Number of columns in the grid\n",
    "n_rows = (n_features + n_cols - 1) // n_cols  # Calculate required number of rows\n",
    "\n",
    "# Create subplots with a grid layout\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4 * n_rows))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes for easier indexing\n",
    "\n",
    "# Loop through all numerical features and plot their distributions\n",
    "for i, col in enumerate(numeric_features):\n",
    "    ax = axes[i]\n",
    "    data = credit_card_df[col].dropna()\n",
    "    \n",
    "    # Plot histogram with density normalization\n",
    "    n_bins = 0\n",
    "    ax.hist(data, bins=100, density=True, alpha=0.5, color='blue')\n",
    "    ax.set_xlim(data.min(), data.max())  # Set x-axis limits to focus on the range of interest\n",
    "    \n",
    "    # Fit a normal distribution to the data and plot it\n",
    "    mu, std = norm.fit(data)\n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "    y = norm.pdf(x, mu, std)\n",
    "    ax.plot(x, y, 'r-', lw=2)\n",
    "    \n",
    "    # Add vertical lines for mean and median\n",
    "    mean_val = data.mean()\n",
    "    median_val = data.median()\n",
    "    ax.axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "    ax.axvline(median_val, color='blue', linestyle='-.', alpha=0.7, label=f'Median: {median_val:.2f}')\n",
    "    \n",
    "    # Add legend, grid, and style ticks\n",
    "    ax.legend(fontsize='small')\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    \n",
    "    # Set titles and labels\n",
    "    ax.set_title(f'Distribution of {col} with Normal Fit')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "# Remove any unused subplots if n_features is not a multiple of n_cols\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theory of subsequential dataset\n",
    "\n",
    "We found out that each client has a monthly analysis of his credit cards on a range of months that goes from january to august. Here we want to prove that we can fix the remaining missing values for customer ID going in one of the other lines that are i range of january august and fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows per customer\n",
    "rows_per_customer = credit_card_df.groupby('Customer_ID').size()\n",
    "print(\"Average entries per customer:\", rows_per_customer.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this I proved that for each customer we have an average of 8 rows (this is pretty accurate because we have only 1891 missing customer IDs over 100k entries). As the lenght of the range of months between January to August is 8 we can proceed by fixing all the remaining missing customer IDs by fixing before the column 'Month'. Before doing this we need to prove that we have a precise number of rows for each client and this is the same for everyone and the clients are in order.\n",
    "\n",
    "ex. \n",
    "| Customer_ID | Month    |\n",
    "|-------------|----------|\n",
    "| Client 1    | January  |\n",
    "| Client 1    | February |\n",
    "| Client 1    | March    |\n",
    "| Client 1    | April    |\n",
    "| Client 1    | May      |\n",
    "| Client 1    | June     |\n",
    "| Client 1    | July     |\n",
    "| Client 1    | August   |\n",
    "\n",
    "and not something like that\n",
    "\n",
    "| Customer_ID | Month    |\n",
    "|-------------|----------|\n",
    "| Client 1    | January  |\n",
    "| Client 2    | January  |\n",
    "| Client 1    | February |\n",
    "| Client 3    | March    |\n",
    "| Client 1    | June     |\n",
    "| Client 5    | April    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print me the first 8 rows of the dataset\n",
    "print(credit_card_df.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print me the last 8 rows of the dataset\n",
    "print(credit_card_df.tail(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$12500 \\cdot 8 = 100 000$ which is the exact number of our rows, and as we can see from the previous outputs they're ordered as each client is consequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a specific row of the dataset\n",
    "print(credit_card_df.iloc[60120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(credit_card_df.iloc[60121])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after I print the first indexes that are assumed to not be consecutive we can clearly see that we're talking about the same customer as they have the same customer and they have subsequent months so we can proceed by fixing the 'Month' column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
